diff --git a/ETL/load_files.py b/ETL/load_files.py
new file mode 100644
index 0000000000000000000000000000000000000000..c634f62c50ce86f73d3e401137d1f7b7f213edcc
--- /dev/null
+++ b/ETL/load_files.py
@@ -0,0 +1,67 @@
+"""Utilities for persisting pipeline outputs to local parquet files."""
+
+from __future__ import annotations
+
+import os
+from pathlib import Path
+from typing import Mapping
+
+import pandas as pd
+
+
+def _resolve_output_dir() -> Path:
+    """Return the directory where parquet files should be stored.
+
+    The location defaults to ``ETL/outputs`` alongside the source files but can
+    be overridden with the ``FILES_OUTPUT_DIR`` environment variable. Relative
+    paths provided via the environment variable are resolved from the current
+    working directory. The directory is created if it does not already exist.
+    """
+
+    override = os.environ.get("FILES_OUTPUT_DIR")
+    if override:
+        base = Path(override).expanduser()
+        if not base.is_absolute():
+            base = Path.cwd() / base
+    else:
+        base = Path(__file__).resolve().parent / "outputs"
+
+    base.mkdir(parents=True, exist_ok=True)
+    return base
+
+
+def _write_parquet(df: pd.DataFrame, destination: Path) -> None:
+    """Write ``df`` to ``destination`` as a parquet file.
+
+    To keep schema information available even for empty dataframes we always
+    emit a parquet file. Pandas handles empty frames gracefully, so no extra
+    guarding is required beyond ensuring the parent directory exists.
+    """
+
+    # Pandas writes atomically by using a temp file under the hood when
+    # possible, but ensuring the parent directory exists keeps things tidy.
+    destination.parent.mkdir(parents=True, exist_ok=True)
+    df.to_parquet(destination, index=False)
+
+
+def load_to_files(bronze: pd.DataFrame, silver: pd.DataFrame, gold: pd.DataFrame) -> None:
+    """Persist the pipeline outputs to parquet files on disk."""
+
+    output_dir = _resolve_output_dir()
+    datasets: Mapping[str, pd.DataFrame] = {
+        "bronze_daily_tracks": bronze,
+        "silver_artist_market_daily": silver,
+        "gold_artist_global_daily": gold,
+    }
+
+    for name, df in datasets.items():
+        if df is None:
+            # Defensive guard: in practice the pipeline always passes DataFrames
+            # but ``None`` could slip through in ad-hoc usage. Skipping keeps the
+            # function tolerant of such cases without crashing.
+            continue
+        path = output_dir / f"{name}.parquet"
+        _write_parquet(df, path)
+
+
+__all__ = ["load_to_files"]
